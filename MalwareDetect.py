# Load libraries
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics, tree #Import scikit-learn metrics module for accuracy calculation
from sklearn.model_selection import cross_val_score
from sklearn.externals import joblib
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import GridSearchCV
import sklearn.ensemble as ske


#Carregando dados
data = pd.read_csv("dataset.csv", sep="|", low_memory=False)
print (data.shape)
#Retirando colunas nao importantes
X = data.drop(["Name", "md5", "legitimate"], axis=1).values
print (X.shape)
y = data["legitimate"].values
print (y.shape)


#Verificando se ha valores ausentes.
NAs = pd.concat([data.isnull().sum()], axis=1, keys=['data'])
NAs[NAs.sum(axis=1) > 0]

#forest = DecisionTreeClassifier(criterion='entropy', max_depth=50, random_state=1, max_features='auto',min_samples_split=10).fit(X, y)
#forest = DecisionTreeClassifier().fit(X, y)
forest = ske.ExtraTreesClassifier(n_estimators=50).fit(X, y)
model = SelectFromModel(forest, prefit=True)#,threshold=0.03)
X_new = model.transform(X)
nb_features = X_new.shape[1]

X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=1) # 70% treinamento and 30% teste

features = []
indices = np.argsort(forest.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], forest.feature_importances_[indices[f]]))
    
for f in sorted(np.argsort(forest.feature_importances_)[::-1][:nb_features]):
    features.append(data.columns[2+f])

#print features


max_depths = np.linspace(1, 100, 100, endpoint=True)
train_results = []
test_results = []

for max_depth in max_depths:
    
    # Create Decision Tree classifer object
    clf = DecisionTreeClassifier(max_depth=max_depth) 
    # Train Decision Tree Classifer
    clf = clf.fit(X_train,y_train)
    #Predict the response for test dataset
    train_pred = clf.predict(X_train)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    train_results.append(roc_auc)
    
    y_pred = clf.predict(X_test)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    # Add auc score to previous test results
    test_results.append(roc_auc)

from matplotlib.legend_handler import HandlerLine2D

line1, = plt.plot(max_depths, train_results, 'b', label="Train AUC")
line2, = plt.plot(max_depths, test_results, 'r', label="Test AUC")

plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
    
plt.ylabel('AUC score')
plt.xlabel('Tree depth')
plt.show()

min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)
train_results = []
test_results = []

for min_samples_split in min_samples_splits:
    
    # Create Decision Tree classifer object
    clf = DecisionTreeClassifier(min_samples_split=min_samples_split) 
    # Train Decision Tree Classifer
    clf = clf.fit(X_train,y_train)
    #Predict the response for test dataset
    train_pred = clf.predict(X_train)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    train_results.append(roc_auc)
    
    y_pred = clf.predict(X_test)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    # Add auc score to previous test results
    test_results.append(roc_auc)

from matplotlib.legend_handler import HandlerLine2D

line1, = plt.plot(min_samples_splits, train_results, 'b', label="Train AUC")
line2, = plt.plot(min_samples_splits, test_results, 'r', label="Test AUC")

plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
    
plt.ylabel('AUC score')
plt.xlabel('min samples slit')
plt.show()


min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)
train_results = []
test_results = []

for min_samples_leaf in min_samples_leafs:
    
    # Create Decision Tree classifer object
    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf) 
    # Train Decision Tree Classifer
    clf = clf.fit(X_train,y_train)
    #Predict the response for test dataset
    train_pred = clf.predict(X_train)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    train_results.append(roc_auc)
    
    y_pred = clf.predict(X_test)
    
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    # Add auc score to previous test results
    test_results.append(roc_auc)

from matplotlib.legend_handler import HandlerLine2D

line1, = plt.plot(min_samples_leafs, train_results, 'b', label="Train AUC")
line2, = plt.plot(min_samples_leafs, test_results, 'r', label="Test AUC")

plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
    
plt.ylabel('AUC score')
plt.xlabel('min samples leaf')
plt.show()

    
##########################################
# Create Decision Tree classifer 
'''
model=DecisionTreeClassifier(random_state=1)
#hyper parameters set
params = {'max_features': ['auto', 'sqrt', 'log2'],
          'criterion':['entropy'],
          'min_samples_split': [10,11,12,13,14,15], 
          'max_depth':[50,60,70,80,90,100],
          'random_state':[1]}
#
clf = GridSearchCV(model, param_grid=params, n_jobs=-1)
'''
clf = DecisionTreeClassifier(criterion='entropy', max_depth=55, random_state=1, max_features='auto',min_samples_split=10) 

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#print("Best Hyper Parameters:\n",clf.best_params_)


#AUG(Area Under Curve)
false_positive_rate, true_positive_rate, threshlods = roc_curve(y_test, y_pred)
roc_auc = auc(false_positive_rate, true_positive_rate)
print("Accuracy AUG:", roc_auc)
##########################################


#Reference: http://carlosbaia.com/2016/12/24/decision-tree-e-random-forest/
#Cross Validation
scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
print("Accuracy CV:",scores.mean())

#Save the algorithm and the feature list for later predictions
print('Salvando lista de features no diretorio de classificacao')
joblib.dump(clf, 'classifier/classifier.pkl')
open('classifier/features.pkl', 'wb').write(pickle.dumps(features))
print('Salvo')
