# Load libraries
import pandas as pd
import numpy as np
import pickle
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics, tree #Import scikit-learn metrics module for accuracy calculation
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import ExtraTreesClassifier
import sklearn.ensemble as ske
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.externals import joblib
from sklearn.feature_selection import SelectFromModel


#import matplotlib.mlab as mlab
#import matplotlib.pyplot as plt


data = pd.read_csv("data.csv", sep="|")
data.head()
#legit_binaries = data[0:41323].drop(['legitimate'], axis=1)
#malicious_binaries = data[41323::].drop(['legitimate'], axis=1)

#plt.hist([legit_binaries['SectionsMaxEntropy'], malicious_binaries['SectionsMaxEntropy']], range=[0,8], normed=True, color=["green", "red"],label=["legitimate", "malicious"])

#plt.show()

X = data.drop(["Name", "md5", "legitimate"], axis=1).values
y = data["legitimate"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test

# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=4, random_state=1)

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

#Reference: http://carlosbaia.com/2016/12/24/decision-tree-e-random-forest/
#Cross Validation
#Servira para validar o modelo.
scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
print("Accuracy:",scores.mean())


model = SelectFromModel(clf, prefit=True)
X_novo = model.transform(X)
nb_features = X_novo.shape[1]

features = []

indices = np.argsort(clf.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], clf.feature_importances_[indices[f]]))
    
for f in sorted(np.argsort(clf.feature_importances_)[::-1][:nb_features]):
    features.append(data.columns[2+f])

#Save the algorithm and the feature list for later predictions
print('Saving algorithm and feature list in classifier directory...')
joblib.dump(clf, 'classifier/classifier.pkl')
open('classifier/features.pkl', 'w').write(pickle.dumps(features))
print('Saved')
