# Load libraries
import pandas as pd
import numpy as np
import pickle
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics, tree #Import scikit-learn metrics module for accuracy calculation
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import ExtraTreesClassifier
import sklearn.ensemble as ske
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.externals import joblib
from sklearn.feature_selection import SelectFromModel
from sklearn.datasets import make_classification
from sklearn.naive_bayes import GaussianNB

#import matplotlib.mlab as mlab
#import matplotlib.pyplot as plt


data = pd.read_csv("dataset.csv", sep="|", low_memory=False)
#data = data.loc[0:82648]
#legit_binaries = data[0:41323].drop(['legitimate'], axis=1)
#malicious_binaries = data[41323::].drop(['legitimate'], axis=1)

#plt.hist([legit_binaries['SectionsMaxEntropy'], malicious_binaries['SectionsMaxEntropy']], range=[0,8], normed=True, color=["green", "red"],label=["legitimate", "malicious"])

#plt.show()
X = data.drop(["Name", "md5", "legitimate"], axis=1).values
y = data["legitimate"].values


#forest = ExtraTreesClassifier(random_state=0).fit(X, y)
#forest = RandomForestClassifier(n_estimators=50,random_state=0).fit(X,y)
forest = DecisionTreeClassifier(criterion='entropy', max_depth=50, random_state=1, max_features='auto',min_samples_split=10).fit(X, y)
model = SelectFromModel(forest, prefit=True)#,threshold=0.03)
X_new = model.transform(X)
nb_features = X_new.shape[1]

X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=1) # 70% treinamento and 30% teste

features = []
indices = np.argsort(forest.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], forest.feature_importances_[indices[f]]))
    
for f in sorted(np.argsort(forest.feature_importances_)[::-1][:nb_features]):
    features.append(data.columns[2+f])

# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion='entropy', max_depth=50, random_state=1, max_features='auto',min_samples_split=10) 
#clf = RandomForestClassifier(random_state=0,criterion='entropy',max_depth=50,n_estimators=150,n_jobs=-1)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

#Reference: http://carlosbaia.com/2016/12/24/decision-tree-e-random-forest/
#Cross Validation
scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
print("Accuracy:",scores.mean())

#Save the algorithm and the feature list for later predictions
print('Salvando lista de features no diretorio de classificacao')
joblib.dump(clf, 'classifier/classifier.pkl')
open('classifier/features.pkl', 'w').write(pickle.dumps(features))
print('Salvo')
